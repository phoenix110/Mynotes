k8s
```
总结概括为以下10条：

1.  不要直接部署裸的Pod。
2.  为工作负载选择合适的Controller。
3.  使用Init容器确保应用程序被正确的初始化。
4.  在应用程序工作负载启动之前先启动service。
5.  使用Deployment history来回滚到历史版本。
6.  使用ConfigMap和Secret来存储配置。
7.  在Pod里增加Readiness和Liveness探针。
8.  给Pod这只CPU和内存资源限额。
9.  定义多个namespace来限制默认service范围的可视性。
10.  配置HPA来动态扩展无状态工作负载。
```
master/node
+ master:API Server,Scheduler,Controller-Manager
+ node: kubelet,容器引擎(docker)，kube-proxy

Pod,Label,Label Selector
+ Label:key=value
+ label Selector:

Pod:
+ 自主Pod
+ 控制器管理的Pod
    + RelicationController 最早的。可以滚动更新和回滚。
    + ReplicaSet 新一代的。1用户期待的pod副本有几个，2标签选择器，3现存pod不够，它会新建。不建议直接使用，而是deployment。deployment
    + Deployment  声明式 管理无状态 用的最多，还支持滚动更新和回滚.建构在RS之上的ReplicaSet，还能控制更新节奏 
deployment---replicaset---pod
kubectl explain deploy
pod数量可以大于节点数量，pod和节点没有对应关系
    + statefulSet  有状态副本集。每一个pod都是被单独管理，独有标识和独有数据集，在被加时需要数据在。比如redis cluster,mysql主从，mongo cluster。非常麻烦，没法抽取个共同策略，只能单独对待。
CDR：Custom Defined Resources,1.8+
Operator:
Helm:外壳，helm install
    + DaemonSet  需要在每一个node运行一个，而不是随意运行（实现系统级的），一般是无状态的，必须是守护状态的
    + Job,Ctonjob 作业，周期性作业。任务完成就释放

分别用于确保不同类型的pod资源，来符合用户期望的方式进行运行。
HPA-HorizontalPodAuto

## **k8s为每一组提供同类服务的pod和他的客户端之间添加了一个中间层，叫service**
只要不删除service，它就会一直在
只要你创建的pod标签label相同

节点网络到集群网络到Pod网络
同一pod内的多个容器间：lo
各pod之间的通信
pod与service之间的通信

CNI
    + flannel:网络配置
    + calico:网络配置，网络策略
    + canel :前两种搭配使用
rpm -ql 查看安装后有哪些目录
## k8s 资源清单定义入门
# 大部分资源的配置清单都有5个部分：
## apiVersion group/version 
$ kubectl api-version
## kind:资源类别
## metadata：元数据，嵌套的字段，二级，三级
+ name:
+ namespace:
+ labels:
+ annotations:(注释)
    每个的资源的引用PATH
        /api/group/version/namespace/type/name
## spec: 我们接下来创建的资源对象，应该满足什么样的规范（然后靠控制器来）disired state
## status:当前这个资源的当前状态（只读,由k8s集群维护）
    kubectl explain pods 查看pods怎么定义
    kubectl explain pods.metadate
    kubectl explain pods.spec
    kubectl explain 
# 创建资源的方法
    apiserver 仅接受JSON格式的资源定义;
    yaml格式提供配置清单，apiserver可自动将其转为json格式，然后再提交;
pod资源
+ spec.containers <[]object>
        - name <string>
          image <string>
          imagePullPolicy <string>镜像pull策略
            Always,Never,IfNotPresent 总是下载，总是不下载，本地有就不下载
+ 修改镜像中的默认应用：
        command,args
            https:kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/
+ 标签：
            key=value
                key:字母，数字，_,-,.五种。只能字母数字开头
                value:可以为空，只能以字母或数字开头及结尾，中间可使用_ - .
    kubectl get pods -l app 过滤标签
+ 标签选择器：
        等值关系：=，==,!=,
        集合关系：
              KEY in (VALUE1,VALUE2,...)
              KEY not in (VALUE1,VALUE2,...)
+ 节点选择器nodeSelector
+ nodeName直接指定运行在哪个节点
+ annotations:
       与label不同的地方在于，它不能用于挑选资源对象，仅用于为对象提供“元数据”
+ Pod的生命周期
    初始化容易
    容器探测
+ restartPolicy:
    Always,OnFailure,Never,Default to Always
+ 探针类型有三种：livenessProbe健康检测，readiness就绪检测，liftcycle?
    ExecAction,TCPSocketAction,HTTPGetAction
```
apiVersion: v1
kind: Pod
metadata:
name: liveness-exec-pod
namespace: default
spec:
containers:
- name liveness-exec-container
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh","-c","touch /tmp/healthy; sleep 20; rm -f /tmp/healthy; sleep 3600"]
    livenessProbe:
    exec:
        command: ["test","-e","tmp/healthy"]
    initialDelaySeconds: 1
    periodSeconds: 3
```
```
apiVersion: v1
kind: Pod
metadata:
name: readiness-httpget-pod
namespace: default
spec:
containers:
- name: readiness-httpget-container
    image: ikubernetes/myapp:v1
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    readinessProbe:
      httpGet:
        port: http
        path: /index.html
    initialDelaySeconds: 1
    periodSeconds: 3
```
## Pod控制器
+ rs replicaSet   kubectl explain rs
模板中定义的标签一定要符合标签选择器定义的
rs-demo.yaml
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
         name: myapp
         namespace: default
spec:
       replicas: 2
       selector:
              matchLabels:
                      app: myapp
                      release: canary
       template:
               metadata:
                        name: myapp-pod
                        labels:
                               app: myapp
                               release: canary
                               environment: qa
                spec:
                        container
                        - name: myapp-container
                          image: ikubernetes/myapp:v1
                          ports:
                          - name: http
                            containerPort: 80
```
## deployment-demo.yaml可以管理 Pod 的多个副本，并确保 Pod 按照期望的状态运行。
## 典型的用例如下：

- 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。
- 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。
- 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。
- 扩容Deployment以满足更高的负载。
- 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。
- 根据Deployment 的状态判断上线是否hang住了。
- 清除旧的不必要的 ReplicaSet。
```
deployment控制replicaset控制pods
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:（要匹配的标签为）
      app: myapp
      release: canary
  template:（使用模板来创建pod）
    metadata:
      labels:（一定要匹配上面标签选择器的标签）
        app: myapp
        release: canary
    spec:（pod的spec）
        containers：
        - name: myapp
          image: ikubernetes/myapp:v1
          ports:
          - name: http
            containerPort: 80
```
## DaemonSet*确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。

## 使用 DaemonSet 的一些典型用法：

*   运行集群存储 daemon，例如在每个 Node 上运行`glusterd`、`ceph`。
*   在每个 Node 上运行日志收集 daemon，例如`fluentd`、`logstash`。
*   在每个 Node 上运行监控 daemon，例如[Prometheus Node Exporter](https://github.com/prometheus/node_exporter)、`collectd`、Datadog 代理、New Relic 代理，或 Ganglia`gmond`。

一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的内存、CPU要求。
ds-demo.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
    name: redis
    namespace: default
spec:
    replicas: 1
    selector:
        matchLabels:
            app: redis
            role: logstor
template:
    metadata:
        labels:
            app: redis
            role: logstor
    spec:（pod的）
        containers:
        - name: redis
          image: redis:4.0-alpline
          ports:
          - name: redis
            containerPort: 6379
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat-ds
  namespace: default
spec:
  selector:
    matchLabels:
      app: filebeat
      release: stable
  template:
    metadata:
      labels:
        app: filebeat
        release: stable
    spec:
      containers:
      -name: filebeat
      image: ikubernetes/filebeat:5.6.5-alpine
      env:
      - name: REDIS_HOST
        value: redis.default.svc.cluster.local
      - name: REDIS_LOG_LEVEL
        value: info
```
kubectl apply -f ds-demo.yaml
kubectl expose deplyment redis --port=6379  ????????pod间通过service调用